{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_dir = 'checkpoints'\n",
    "\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "# model checkpoints - should point to checkpoint_final.pth\n",
    "simmim_model_path = f'{model_dir}/simmim_checkpoint_final.pth'\n",
    "kamim_model_path = f'{model_dir}/kamim_checkpoint_final.pth'\n",
    "\n",
    "IMG_NET_PATH = '../../Datasets/Imagenet/val/'        # path to imagenet validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'batch_size': 2048,\n",
    "    'model_name': 'ViT-B',\n",
    "    'dimension' : 192,\n",
    "    'model_patch_size' : 16,\n",
    "    'masking_patch_size' :32,\n",
    "    'mask_ratio': 0.6,\n",
    "    'num_hidden_layer': 12,\n",
    "    'num_attention_heads': 12,\n",
    "    'hidden_size': 768,\n",
    "    'intermediate_size': 3072, \n",
    "    \n",
    "    'batch_size_finetuning': 2048,\n",
    "    'batch_size_linear_probe': 2048,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = parameters['dimension']                                 # dimension of image\n",
    "MODEL_PATCH_SIZE = parameters['model_patch_size']                   # patch size         \n",
    "MASKING_PATCH_SIZE = parameters['masking_patch_size']               # masking patch size\n",
    "MASK_RATIO = parameters['mask_ratio']                               # model masking ratio\n",
    "INTERMEDIATE_SIZE = parameters['intermediate_size']\n",
    "# model config\n",
    "HIDDEN_SIZE = parameters['hidden_size']\n",
    "HIDDEN_LAYERS = parameters['num_hidden_layer']\n",
    "ATTN_HEADS = parameters['num_attention_heads']\n",
    "\n",
    "config = transformers.ViTConfig(\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    num_hidden_layers = HIDDEN_LAYERS,\n",
    "    num_attention_heads= ATTN_HEADS,\n",
    "    image_size = DIMENSION,\n",
    "    patch_size = MODEL_PATCH_SIZE,\n",
    "    intermediate_size= INTERMEDIATE_SIZE,\n",
    "    num_channels= 3,\n",
    ")\n",
    "model_simmim = transformers.ViTForMaskedImageModeling(config)\n",
    "chkpt = torch.load(simmim_model_path)\n",
    "if type(chkpt) == dict and 'model_state_dict' in chkpt.keys():\n",
    "    model_simmim.load_state_dict(chkpt['model_state_dict'])\n",
    "else:\n",
    "    model_simmim.load_state_dict(chkpt)\n",
    "model_simmim = model_simmim.to(DEVICE)\n",
    "\n",
    "model_kamim = transformers.ViTForMaskedImageModeling(config)\n",
    "chkpt = torch.load(kamim_model_path)\n",
    "if type(chkpt) == dict and 'model_state_dict' in chkpt.keys():\n",
    "    model_kamim.load_state_dict(chkpt['model_state_dict'])\n",
    "else:\n",
    "    model_kamim.load_state_dict(chkpt)\n",
    "model_kamim = model_kamim.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "transform = v2.Compose([\n",
    "                    v2.Normalize(\n",
    "                        mean = [0.485, 0.456, 0.406],\n",
    "                        std =  [0.229, 0.224, 0.225]\n",
    "                    )\n",
    "                ])\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):\n",
    "        self.input_size = input_size\n",
    "        self.mask_patch_size = mask_patch_size\n",
    "        self.model_patch_size = model_patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "        assert self.input_size % self.mask_patch_size == 0\n",
    "        assert self.mask_patch_size % self.model_patch_size == 0\n",
    "        \n",
    "        self.rand_size = self.input_size // self.mask_patch_size\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size\n",
    "        \n",
    "        self.token_count = self.rand_size ** 2\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "        \n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[:self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "        \n",
    "        mask = mask.reshape((self.rand_size, self.rand_size))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "class ImageNet_Custom(torchvision.datasets.ImageFolder):\n",
    "    def __init__(self,\n",
    "                 src,\n",
    "                 dimension = (192,192),\n",
    "                 mask_patch_size = 32,\n",
    "                 model_patch_size = 16,\n",
    "                 mask_ratio = 0.6):\n",
    "        super().__init__(src)\n",
    "        self.mask_gen = MaskGenerator(input_size = dimension[0],\n",
    "                                      mask_patch_size = mask_patch_size,\n",
    "                                      model_patch_size = model_patch_size,\n",
    "                                      mask_ratio = mask_ratio\n",
    "                                      )\n",
    "        self.transform = v2.Compose([\n",
    "                            v2.ToImage(), \n",
    "                            v2.ToDtype(torch.uint8, scale=True),\n",
    "                            v2.Compose([\n",
    "                                v2.Resize((192, 192), antialias = False),\n",
    "                                v2.ToDtype(torch.float32, scale=True)\n",
    "                                ]),\n",
    "                            ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = super().__getitem__(idx)        \n",
    "        mask = self.mask_gen()\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgnet_train = ImageNet_Custom(\n",
    "    src = IMG_NET_PATH,\n",
    "    dimension = (192,192),\n",
    "    mask_patch_size = 32,\n",
    "    model_patch_size = 16,\n",
    "    mask_ratio = 0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    imgnet_train,\n",
    "    batch_size = 512,\n",
    "    shuffle = False,\n",
    "    num_workers = 12,\n",
    "    pin_memory = True,\n",
    "    persistent_workers = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "dataloader_shuffle = torch.utils.data.DataLoader(\n",
    "    imgnet_train,\n",
    "    batch_size = 512,\n",
    "    shuffle = True,\n",
    "    num_workers = 12,\n",
    "    pin_memory = True,\n",
    "    persistent_workers = True,\n",
    ")\n",
    "\n",
    "# for reconstructed images\n",
    "_, data = next(enumerate(dataloader_shuffle))\n",
    "plain_img, mask = data\n",
    "\n",
    "img = transform(plain_img).to(DEVICE)\n",
    "mask = mask.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reproduc_simmim = model_simmim(img, bool_masked_pos=mask.flatten(1))['reconstruction']\n",
    "    reproduc_kamim = model_kamim(img, bool_masked_pos=mask.flatten(1))['reconstruction']\n",
    "    \n",
    "w_img =  (\n",
    "    mask.repeat_interleave(16, 1)\n",
    "    .repeat_interleave(16, 2)\n",
    "    .unsqueeze(1)\n",
    "    .contiguous()\n",
    "    ).cpu()\n",
    "\n",
    "masked_image = w_img.cpu() * plain_img.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "si = 5\n",
    "step = 25\n",
    "\n",
    "max_imgs = 5\n",
    "fig, ax = plt.subplots(4, max_imgs, figsize = (5,4), dpi = 300)\n",
    "\n",
    "for i in range(max_imgs):\n",
    "    print(si+step*i)\n",
    "    ax[0][i].imshow(plain_img[si+step*i].permute(1,2,0).cpu())\n",
    "    ax[1][i].imshow(masked_image[si+step*i].permute(1,2,0).cpu())\n",
    "    ax[2][i].imshow(unorm(reproduc_simmim[si+step*i]).permute(1,2,0).cpu())\n",
    "    ax[3][i].imshow(unorm(reproduc_kamim[si+step*i]).permute(1,2,0).cpu())\n",
    "\n",
    "ax[0][0].set_ylabel(\"Reference\", size = 8)\n",
    "ax[1][0].set_ylabel(\"Masked Image\", size = 8)\n",
    "ax[2][0].set_ylabel(\"SimMIM\", size = 8)\n",
    "ax[3][0].set_ylabel(\"KAMIM\", size = 8)\n",
    "\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    for _ in a:\n",
    "        _.tick_params(left = False, right = False , labelleft = False , \n",
    "                labelbottom = False, bottom = False)\n",
    "        _.xaxis.set_ticklabels([])\n",
    "        _.yaxis.set_ticklabels([])\n",
    "        _.set_axis_on()\n",
    "        \n",
    "fig.tight_layout(pad = 0.5)\n",
    "fig.savefig(\"reconstruction.pdf\", facecolor = 'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init psnr\n",
    "psnr_simmim = torchmetrics.image.PeakSignalNoiseRatio().to(DEVICE)\n",
    "psnr_kamim = torchmetrics.image.PeakSignalNoiseRatio().to(DEVICE)\n",
    "\n",
    "# init ssim\n",
    "ssim_simmim = torchmetrics.image.StructuralSimilarityIndexMeasure().to(DEVICE)\n",
    "ssim_kamim = torchmetrics.image.StructuralSimilarityIndexMeasure().to(DEVICE)\n",
    "\n",
    "for data in tqdm(dataloader):\n",
    "    img, mask = data\n",
    "    img = transform(img)\n",
    "    \n",
    "    img = img.to(DEVICE)\n",
    "    mask = mask.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reproduc_simmim = model_simmim(img, bool_masked_pos=mask.flatten(1))['reconstruction']\n",
    "        reproduc_kamim = model_kamim(img, bool_masked_pos=mask.flatten(1))['reconstruction']\n",
    "\n",
    "        # update_psnr\n",
    "        psnr_simmim(reproduc_simmim, img)\n",
    "        psnr_kamim(reproduc_kamim, img)\n",
    "        \n",
    "        # update ssim\n",
    "        ssim_simmim(reproduc_simmim, img)\n",
    "        ssim_kamim(reproduc_kamim, img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On training set (obtained by changing the directory to Imagenet/train)\n",
    "\n",
    "- SimMIM PSNR: 12.082398414611816\n",
    "- SimMIM SSIM: 0.11498011648654938\n",
    "- KAMIM PSNR: 12.088289260864258\n",
    "- KAMIM SSIM: 0.11290539056062698\n",
    "\n",
    "On testing set\n",
    "\n",
    "- SimMIM PSNR: 12.058865547180176\n",
    "- SimMIM SSIM: 0.10958657413721085\n",
    "- KAMIM PSNR: 12.063213348388672\n",
    "- KAMIM SSIM: 0.10762996971607208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SimMIM PSNR: {psnr_simmim.compute()}\")\n",
    "print(f\"SimMIM SSIM: {ssim_simmim.compute()}\")\n",
    "\n",
    "print(f\"KAMIM PSNR: {psnr_kamim.compute()}\")\n",
    "print(f\"KAMIM SSIM: {ssim_kamim.compute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
